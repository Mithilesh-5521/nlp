--------------------------------------------------------------------------------------------------ASSIGNMENT-1--------------------------------------------------------------------------------------------------

Perform tokenization (Whitespace, Punctuation-based, Treebank, Tweet, MWE) using NLTK library. Use porter stemmer and snowball stemmer for stemming. Use any technique for lemmatization.

CODE:

import nltk
from nltk.tokenize import word_tokenize, wordpunct_tokenize, TreebankWordTokenizer
from nltk.tokenize import TweetTokenizer, MWETokenizer
from nltk.stem import PorterStemmer, SnowballStemmer
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# Sample text for demonstration
sample_text = "NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum. Running on Python 2.7 and Python 3.5+."

print("Original Text:")
print(sample_text)
print("\n" + "="*80 + "\n")

# 1. TOKENIZATION
print("1. TOKENIZATION\n")

# Whitespace tokenization
print("Whitespace Tokenization:")
whitespace_tokens = sample_text.split()
print(whitespace_tokens)
print(f"Number of tokens: {len(whitespace_tokens)}")
print("\n" + "-"*40 + "\n")

# Punctuation-based tokenization (wordpunct_tokenize)
print("Punctuation-based Tokenization:")
punct_tokens = wordpunct_tokenize(sample_text)
print(punct_tokens)
print(f"Number of tokens: {len(punct_tokens)}")
print("\n" + "-"*40 + "\n")

# Treebank tokenization
print("Treebank Tokenization:")
treebank_tokenizer = TreebankWordTokenizer()
treebank_tokens = treebank_tokenizer.tokenize(sample_text)
print(treebank_tokens)
print(f"Number of tokens: {len(treebank_tokens)}")
print("\n" + "-"*40 + "\n")

# Tweet tokenization
print("Tweet Tokenization:")
tweet_tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)
tweet_tokens = tweet_tokenizer.tokenize(sample_text)
print(tweet_tokens)
print(f"Number of tokens: {len(tweet_tokens)}")
print("\n" + "-"*40 + "\n")

# Multi-Word Expression (MWE) tokenization
print("Multi-Word Expression Tokenization:")
mwe_tokenizer = MWETokenizer([('NLTK', 'is'), ('text', 'processing')])
# First tokenize with a standard method, then apply MWE
pre_tokens = word_tokenize(sample_text)
mwe_tokens = mwe_tokenizer.tokenize(pre_tokens)
print(mwe_tokens)
print(f"Number of tokens: {len(mwe_tokens)}")
print("\n" + "="*80 + "\n")

# 2. STEMMING
print("2. STEMMING\n")

# Porter Stemmer
print("Porter Stemmer:")
porter = PorterStemmer()
porter_stems = [porter.stem(token) for token in word_tokenize(sample_text)]
print(porter_stems)
print("\n" + "-"*40 + "\n")

# Snowball Stemmer (Porter2)
print("Snowball Stemmer (English):")
snowball = SnowballStemmer('english')
snowball_stems = [snowball.stem(token) for token in word_tokenize(sample_text)]
print(snowball_stems)
print("\n" + "="*80 + "\n")

# 3. LEMMATIZATION
print("3. LEMMATIZATION\n")

# Basic lemmatization with WordNet lemmatizer
print("Basic WordNet Lemmatization:")
lemmatizer = WordNetLemmatizer()
basic_lemmas = [lemmatizer.lemmatize(token) for token in word_tokenize(sample_text)]
print(basic_lemmas)
print("\n" + "-"*40 + "\n")

--------------------------------------------------------------------------------------------------ASSIGNMENT-2--------------------------------------------------------------------------------------------------

2. Perform bag-of-words approach (count occurrence, normalized count occurrence), TF-IDF on data. Create embeddings using Word2Vec

CODE:

!pip install --upgrade --force-reinstall numpy==1.24.4 scipy==1.10.1 scikit-learn==1.2.2 gensim==4.3.2
import os
os.kill(os.getpid(), 9)  # Restart the Colab runtime to apply changes

# Install gensim (for Word2Vec) if not already installed
!pip install -q gensim

# Import necessary libraries
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.preprocessing import normalize
from gensim.models import Word2Vec
import numpy as np
import pandas as pd

# Sample text corpus (replace this with your own data)
corpus = [
    "The cat sat on the mat.",
    "Dogs and cats are friends.",
    "The dog chased the cat.",
    "Cats sleep on soft mats.",
    "Dogs bark loudly at night."
]

# Create CountVectorizer instance
count_vectorizer = CountVectorizer()
X_counts = count_vectorizer.fit_transform(corpus)

# Convert to DataFrame
df_counts = pd.DataFrame(X_counts.toarray(), columns=count_vectorizer.get_feature_names_out())
print("Raw Count (Bag-of-Words):")
df_counts

# Normalize the raw count vectors
X_normalized = normalize(X_counts, norm='l1', axis=1)

# Convert to DataFrame
df_normalized = pd.DataFrame(X_normalized.toarray(), columns=count_vectorizer.get_feature_names_out())
print("Normalized Count (Bag-of-Words):")
df_normalized

# Normalize the raw count vectors
X_normalized = normalize(X_counts, norm='l1', axis=1)

# Convert to DataFrame
df_normalized = pd.DataFrame(X_normalized.toarray(), columns=count_vectorizer.get_feature_names_out())
print("Normalized Count (Bag-of-Words):")
df_normalized

# Create TF-IDF vectorizer instance
tfidf_vectorizer = TfidfVectorizer()
X_tfidf = tfidf_vectorizer.fit_transform(corpus)

# Convert to DataFrame
df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())
print("TF-IDF Representation:")
df_tfidf

# Preprocess corpus into tokens for Word2Vec
tokenized_corpus = [sentence.lower().replace(".", "").split() for sentence in corpus]

# Train Word2Vec model
w2v_model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)

# Example: Get vector for a word
print("Vector for 'cat':")
print(w2v_model.wv['cat'])


def document_vector(doc):
    # Remove out-of-vocabulary words
    words = [word for word in doc if word in w2v_model.wv]
    return np.mean(w2v_model.wv[words], axis=0)

# Apply to each document
doc_vectors = np.array([document_vector(doc) for doc in tokenized_corpus])

# Convert to DataFrame
df_doc_vectors = pd.DataFrame(doc_vectors)
print("Document Embeddings using Word2Vec:")
df_doc_vectors


--------------------------------------------------------------------------------------------------ASSIGNMENT-3--------------------------------------------------------------------------------------------------

Perform bag-of-words approach (count occurrence, normalized count occurrence), TF-IDF on data. Create embeddings using Word2Vec,



CODE:

!pip install -q spacy nltk scikit-learn joblib
!python -m spacy download en_core_web_sm

 import pandas as pd
import numpy as np
import nltk
import spacy
import re
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
import joblib

nltk.download('stopwords')
nltk.download('punkt')

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

stop_words = set(stopwords.words('english'))
nlp = spacy.load("en_core_web_sm")

# Sample dataset (replace this with your actual data source)
data = {
    'text': [
        "Dogs are running in the park!",
        "The quick brown fox jumps over the lazy dog.",
        "Cats are sleeping under the sun.",
        "Birds are flying high in the sky."
    ],
    'label': ['animal', 'animal', 'animal', 'animal']
}
df = pd.DataFrame(data)

def clean_text(text):
    # Lowercase and remove non-alphabetic characters
    text = text.lower()
    text = re.sub(r'[^a-z\s]', '', text)
    return text

def preprocess(text):
    text = clean_text(text)
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word not in stop_words]
    doc = nlp(" ".join(tokens))
    lemmas = [token.lemma_ for token in doc if token.lemma_ not in stop_words]
    return " ".join(lemmas)


# Apply preprocessing
df['clean_text'] = df['text'].apply(preprocess)
df[['text', 'clean_text']]



le = LabelEncoder()
df['label_encoded'] = le.fit_transform(df['label'])

# Save label encoder
joblib.dump(le, 'label_encoder.joblib')
df[['label', 'label_encoded']]

tfidf = TfidfVectorizer()
X_tfidf = tfidf.fit_transform(df['clean_text'])

# Convert to DataFrame
df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out())

# Save TF-IDF model
joblib.dump(tfidf, 'tfidf_vectorizer.joblib')

# Show TF-IDF
df_tfidf.head()


# Save TF-IDF matrix and processed data
df.to_csv("processed_data.csv", index=False)
df_tfidf.to_csv("tfidf_matrix.csv", index=False)
print("Saved: processed_data.csv, tfidf_matrix.csv, tfidf_vectorizer.joblib, label_encoder.joblib")


#representation

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import seaborn as sns

# Set style for better visuals
sns.set(style="whitegrid")


# Calculate average TF-IDF score for each word across all documents
avg_tfidf_scores = np.asarray(X_tfidf.mean(axis=0)).flatten()
vocab = tfidf.get_feature_names_out()
tfidf_scores = pd.DataFrame({'word': vocab, 'avg_score': avg_tfidf_scores})

# Sort by highest average TF-IDF score
top_n = 20  # You can change this number
top_words = tfidf_scores.sort_values(by='avg_score', ascending=False).head(top_n)

# Plot
plt.figure(figsize=(10, 6))
sns.barplot(data=top_words, x='avg_score', y='word', palette='viridis')
plt.title(f"Top {top_n} Words by Average TF-IDF Score")
plt.xlabel("Average TF-IDF Score")
plt.ylabel("Word")
plt.show()


--------------------------------------------------------------------------------------------------ASSIGNMENT-4--------------------------------------------------------------------------------------------------

4. Create a transformer from scratch using the Pytorch library google colab

CODE:
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)  # (1, max_len, d_model)
        self.register_buffer('pe', pe)

  def forward(self, x):
        x = x + self.pe[:, :x.size(1), :]
        return x


        
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        assert d_model % num_heads == 0
        self.d_k = d_model // num_heads
        self.num_heads = num_heads

   self.q_linear = nn.Linear(d_model, d_model)
   self.k_linear = nn.Linear(d_model, d_model)
        self.v_linear = nn.Linear(d_model, d_model)
        self.out = nn.Linear(d_model, d_model)

   def forward(self, q, k, v, mask=None):
        B, T, D = q.size()
        q = self.q_linear(q).view(B, T, self.num_heads, self.d_k).transpose(1, 2)
        k = self.k_linear(k).view(B, T, self.num_heads, self.d_k).transpose(1, 2)
        v = self.v_linear(v).view(B, T, self.num_heads, self.d_k).transpose(1, 2)

   scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)
      if mask is not None:
          scores = scores.masked_fill(mask == 0, -1e9)
      attn = F.softmax(scores, dim=-1)

  out = torch.matmul(attn, v).transpose(1, 2).contiguous().view(B, T, D)
  return self.out(out)

  class TransformerBlock(nn.Module):
    def __init__(self, d_model, num_heads, ff_hidden_dim, dropout=0.1):
        super().__init__()
        self.attn = MultiHeadAttention(d_model, num_heads)
        self.norm1 = nn.LayerNorm(d_model)
        self.ff = nn.Sequential(
            nn.Linear(d_model, ff_hidden_dim),
            nn.ReLU(),
            nn.Linear(ff_hidden_dim, d_model)
        )
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

  def forward(self, x, mask=None):
      attn_out = self.attn(x, x, x, mask)
       x = self.norm1(x + self.dropout(attn_out))
       ff_out = self.ff(x)
       x = self.norm2(x + self.dropout(ff_out))
      return x


class TransformerEncoder(nn.Module):
    def __init__(self, vocab_size, d_model=128, num_heads=4, num_layers=2, ff_hidden_dim=512, max_len=100, dropout=0.1):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model, max_len)
        self.layers = nn.ModuleList([
            TransformerBlock(d_model, num_heads, ff_hidden_dim, dropout)
        for _ in range(num_layers)])
        self.dropout = nn.Dropout(dropout)

  def forward(self, src, mask=None):
      x = self.embedding(src)
      x = self.pos_encoder(x)
     for layer in self.layers:
           x = layer(x, mask)
     return x



# Dummy test data: batch of token sequences
batch_size = 2
seq_len = 10
vocab_size = 1000
dummy_input = torch.randint(0, vocab_size, (batch_size, seq_len))

# Create and run the Transformer
model = TransformerEncoder(vocab_size)
output = model(dummy_input)

print("Input shape:", dummy_input.shape)
print("Output shape:", output.shape)  # Should be (batch_size, seq_len, d_model)


--------------------------------------------------------------------------------------------------ASSIGNMENT-5--------------------------------------------------------------------------------------------------
Perform different parsing techniques using Shallow parser, regex parser.

import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')
# Download the 'averaged_perceptron_tagger_eng' package
nltk.download('averaged_perceptron_tagger_eng') # This line is added to download the missing package
from nltk import pos_tag, word_tokenize


from nltk import pos_tag, word_tokenize, ne_chunk
from nltk.chunk import RegexpParser

sentence = "The quick brown fox jumped over the lazy dog near New York City."

# Tokenize and tag
tokens = word_tokenize(sentence)
tagged = pos_tag(tokens)

# Define a chunk grammar: NP (noun phrase) is a DT (determiner) followed by any number of adjectives (JJ) and a noun (NN)
chunk_grammar = "NP: {<DT>?<JJ>*<NN>}"

# Create parser and parse
chunk_parser = RegexpParser(chunk_grammar)
tree = chunk_parser.parse(tagged)

# Instead of tree.draw(), print the tree structure
print(tree)


#REGEX PARSING

import re

text = "John was born on 12/08/1990 and his sister on 04/11/1995."

# Extract dates in DD/MM/YYYY format
pattern = r'\b\d{2}/\d{2}/\d{4}\b'
dates = re.findall(pattern, text)

print("Extracted dates:", dates)

text2 = "You can reach us at (123) 456-7890 or 987-654-3210."

# Regex pattern to match phone numbers in two common formats
phone_pattern = r'\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}'
phone_numbers = re.findall(phone_pattern, text2)

print("Extracted phone numbers:", phone_numbers)


text3 = "Please contact us at support@example.com or info@mywebsite.org for more details."

# Regex pattern to match email addresses
email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'
emails = re.findall(email_pattern, text3)

print("Extracted emails:", emails)
